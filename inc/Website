Website
	manual punch in of data including original source URL - sends data to database 2
	Offers data dumps for sharing
		and continuous hashes of the whole database

Scraper
	autograbber of content - sends data to database 2
	Crawls existing content looking for links

Tools for website
	autosubmit data
	output meta tag indicating original source

Browser extensions
	User submitted opinions of URLs
	Sends data to database 2
	store
		user name
		URL submitted
		their opinion

Database 1
	Stores hashes and corresponding URLs
	~240 GB for whole internet

Database 2
	Stores meta data from user submitted opinions

Experience
	Ryan
		Archival service for the Norwegian government
			Converted most government websites to PDF format
			Built scalable system to allow them to create more websitse than they could previously
		Scaling of frontend web development
		Browser extensions

	Craig
		military scale up

Remember languages





kB/page
80,000 * 1,600,000,000
128,000 GB storage for all HTML on web
0.0225 * 128,000 (cost to store them all on S3)
US$3000/month








32 * 1.4 billion web pages

average page URL is 77 characterse long

147840000000


148 GB







240 GB
